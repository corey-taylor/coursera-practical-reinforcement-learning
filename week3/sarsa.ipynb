{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPTB73LdGpWX"
      },
      "source": [
        "## On-policy learning and SARSA\n",
        "\n",
        "_This notebook builds upon `qlearning.ipynb`, or to be exact your implementation of QLearningAgent._\n",
        "\n",
        "The policy we're gonna use is epsilon-greedy policy, where agent takes optimal action with probability $(1-\\epsilon)$, otherwise samples action at random. Note that agent __can__ occasionally sample optimal action during random sampling by pure chance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efdKtURHGpWc",
        "outputId": "e25e6c82-7fe6-465a-a5c6-f4f7a9649283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting virtual X frame buffer: Xvfb.\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week3_model_free/submit.py\n",
        "\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jdmpcuKjGpWe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-qrCU4bGpWe"
      },
      "source": [
        "You can copy your `QLearningAgent` implementation from previous notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-f4g9Je9GpWf"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, alpha, epsilon, discount, get_legal_actions):\n",
        "        \"\"\"\n",
        "        Q-Learning Agent\n",
        "        based on http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n",
        "        Instance variables you have access to\n",
        "          - self.epsilon (exploration prob)\n",
        "          - self.alpha (learning rate)\n",
        "          - self.discount (discount rate aka gamma)\n",
        "\n",
        "        Functions you should use\n",
        "          - self.get_legal_actions(state) {state, hashable -> list of actions, each is hashable}\n",
        "            which returns legal actions for a state\n",
        "          - self.get_qvalue(state,action)\n",
        "            which returns Q(state,action)\n",
        "          - self.set_qvalue(state,action,value)\n",
        "            which sets Q(state,action) := value\n",
        "\n",
        "        !!!Important!!!\n",
        "        Note: please avoid using self._qValues directly. \n",
        "            There's a special self.get_qvalue/set_qvalue for that.\n",
        "        \"\"\"\n",
        "\n",
        "        self.get_legal_actions = get_legal_actions\n",
        "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon\n",
        "        self.discount = discount\n",
        "\n",
        "    def get_qvalue(self, state, action):\n",
        "        \"\"\" Returns Q(state,action) \"\"\"\n",
        "        return self._qvalues[state][action]\n",
        "\n",
        "    def set_qvalue(self,state,action,value):\n",
        "        \"\"\" Sets the Qvalue for [state,action] to the given value \"\"\"\n",
        "        self._qvalues[state][action] = value\n",
        "\n",
        "    #---------------------START OF YOUR CODE---------------------#\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\"\n",
        "        Compute your agent's estimate of V(s) using current q-values\n",
        "        V(s) = max_over_action Q(state,action) over possible actions.\n",
        "        Note: please take into account that q-values can be negative.\n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        #If there are no legal actions, return 0.0\n",
        "        if len(possible_actions) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        value = max([self.get_qvalue(state, action) for action in possible_actions])\n",
        "\n",
        "        return value\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        \"\"\"\n",
        "        You should do your Q-Value update here:\n",
        "           Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * V(s'))\n",
        "        \"\"\"\n",
        "\n",
        "        #agent parameters\n",
        "        gamma = self.discount\n",
        "        learning_rate = self.alpha\n",
        "\n",
        "        q_value = (1 - learning_rate) * self.get_qvalue(state, action) + learning_rate * (reward + gamma + self.get_value(next_state))\n",
        "        \n",
        "        self.set_qvalue(state, action, q_value)\n",
        "\n",
        "    \n",
        "    def get_best_action(self, state):\n",
        "        \"\"\"\n",
        "        Compute the best action to take in a state (using current q-values). \n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        #If there are no legal actions, return None\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "\n",
        "        best_action = possible_actions[np.argmax([self.get_qvalue(state, action) for action in possible_actions])]\n",
        "\n",
        "        return best_action\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"\n",
        "        Compute the action to take in the current state, including exploration.  \n",
        "        With probability self.epsilon, we should take a random action.\n",
        "            otherwise - the best policy action (self.getPolicy).\n",
        "        \n",
        "        Note: To pick randomly from a list, use random.choice(list). \n",
        "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
        "              and compare it with your probability\n",
        "        \"\"\"\n",
        "\n",
        "        # Pick Action\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "        action = None\n",
        "\n",
        "        #If there are no legal actions, return None\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "\n",
        "        #agent parameters:\n",
        "        epsilon = self.epsilon\n",
        "\n",
        "        random_action = np.random.uniform(0, 1)\n",
        "        if random_action > epsilon:\n",
        "            # take computed action\n",
        "            chosen_action = self.get_best_action(state)\n",
        "        else:\n",
        "            # take random action\n",
        "            chosen_action = random.choice(possible_actions)\n",
        "        \n",
        "        \n",
        "        return chosen_action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7f4nBLcGpWi"
      },
      "source": [
        "Now we gonna implement Expected Value SARSA on top of it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YjYDsyQSGpWi"
      },
      "outputs": [],
      "source": [
        "class EVSarsaAgent(QLearningAgent):\n",
        "    \"\"\" \n",
        "    An agent that changes some of q-learning functions to implement Expected Value SARSA. \n",
        "    Note: this demo assumes that your implementation of QLearningAgent.update uses get_value(next_state).\n",
        "    If it doesn't, please add\n",
        "        def update(self, state, action, reward, next_state):\n",
        "            and implement it for Expected Value SARSA's V(s')\n",
        "    \"\"\"\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\" \n",
        "        Returns Vpi for current state under epsilon-greedy policy:\n",
        "          V_{pi}(s) = sum _{over a_i} {pi(a_i | s) * Q(s, a_i)}\n",
        "\n",
        "        Hint: all other methods from QLearningAgent are still accessible.\n",
        "        \"\"\"\n",
        "        epsilon = self.epsilon\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        # If there are no legal actions, return 0.0\n",
        "        if len(possible_actions) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        state_value = 0\n",
        "        for action in possible_actions:\n",
        "            q_value = self.get_qvalue(state, action)\n",
        "            if action == self.get_best_action(state):\n",
        "                state_value += ((1 - epsilon) + epsilon / len(possible_actions)) * q_value\n",
        "            else:\n",
        "                state_value += epsilon / len(possible_actions) * q_value\n",
        "\n",
        "        return state_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QApj5AHfGpWj"
      },
      "source": [
        "### Cliff World\n",
        "\n",
        "Let's now see how our algorithm compares against q-learning in case where we force agent to explore all the time.\n",
        "\n",
        "<img src=https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/cliffworld.png width=600>\n",
        "<center><i>image by cs188</i></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c-_zXroGpWk",
        "outputId": "1d9dc39c-aa0f-4198-d22c-8398688e96d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    This is a simple implementation of the Gridworld Cliff\n",
            "    reinforcement learning task.\n",
            "\n",
            "    Adapted from Example 6.6 (page 106) from Reinforcement Learning: An Introduction\n",
            "    by Sutton and Barto:\n",
            "    http://incompleteideas.net/book/bookdraft2018jan1.pdf\n",
            "\n",
            "    With inspiration from:\n",
            "    https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py\n",
            "\n",
            "    The board is a 4x12 matrix, with (using Numpy matrix indexing):\n",
            "        [3, 0] as the start at bottom-left\n",
            "        [3, 11] as the goal at bottom-right\n",
            "        [3, 1..10] as the cliff at bottom-center\n",
            "\n",
            "    Each time step incurs -1 reward, and stepping into the cliff incurs -100 reward\n",
            "    and a reset to the start. An episode terminates when the agent reaches the goal.\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import gym.envs.toy_text\n",
        "env = gym.envs.toy_text.CliffWalkingEnv()\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "print(env.__doc__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAGmR9-3GpWk",
        "outputId": "c7bf25bb-959e-469e-c834-b6170fc16e28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Our cliffworld has one difference from what's on the image: there is no wall.\n",
        "# Agent can choose to go as close to the cliff as it wishes. x:start, T:exit, C:cliff, o: flat ground\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3cQ8YYLnGpWk"
      },
      "outputs": [],
      "source": [
        "def play_and_train(env, agent, t_max=10**4):\n",
        "    \"\"\"This function should \n",
        "    - run a full game, actions given by agent.getAction(s)\n",
        "    - train agent using agent.update(...) whenever possible\n",
        "    - return total reward\"\"\"\n",
        "    total_reward = 0.0\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        a = agent.get_action(s)\n",
        "\n",
        "        next_s, r, done, _ = env.step(a)\n",
        "        agent.update(s, a, r, next_s)\n",
        "\n",
        "        s = next_s\n",
        "        total_reward += r\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "az8Ml0_oGpWl"
      },
      "outputs": [],
      "source": [
        "agent_sarsa = EVSarsaAgent(alpha=0.25, epsilon=0.2, discount=0.99,\n",
        "                           get_legal_actions=lambda s: range(n_actions))\n",
        "\n",
        "agent_ql = QLearningAgent(alpha=0.25, epsilon=0.2, discount=0.99,\n",
        "                          get_legal_actions=lambda s: range(n_actions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        },
        "id": "t-lJ3crXGpWl",
        "outputId": "934ec52a-d049-434a-e338-5e3f794045df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVSARSA mean reward = -489.39\n",
            "QLEARNING mean reward = -1020.25\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-68f47e9dd2a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mrewards_sarsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplay_and_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_sarsa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mrewards_ql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplay_and_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_ql\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Note: agent.epsilon stays constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-b6215eb71368>\u001b[0m in \u001b[0;36mplay_and_train\u001b[0;34m(env, agent, t_max)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mnext_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-1b070dc71ad3>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, state, action, reward, next_state)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mq_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_qvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_qvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-1b070dc71ad3>\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_qvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossible_actions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-1b070dc71ad3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_qvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossible_actions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-1b070dc71ad3>\u001b[0m in \u001b[0;36mget_qvalue\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_qvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;34m\"\"\" Returns Q(state,action) \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_qvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "import pandas as pd\n",
        "\n",
        "def moving_average(x, span=100):\n",
        "    return pd.DataFrame({'x': np.asarray(x)}).x.ewm(span=span).mean().values\n",
        "\n",
        "rewards_sarsa, rewards_ql = [], []\n",
        "\n",
        "for i in range(5000):\n",
        "    rewards_sarsa.append(play_and_train(env, agent_sarsa))\n",
        "    rewards_ql.append(play_and_train(env, agent_ql))\n",
        "    # Note: agent.epsilon stays constant\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        clear_output(True)\n",
        "        print('EVSARSA mean reward =', np.mean(rewards_sarsa[-100:]))\n",
        "        print('QLEARNING mean reward =', np.mean(rewards_ql[-100:]))\n",
        "        plt.title(\"epsilon = %s\" % agent_ql.epsilon)\n",
        "        plt.plot(moving_average(rewards_sarsa), label='ev_sarsa')\n",
        "        plt.plot(moving_average(rewards_ql), label='qlearning')\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "        plt.ylim(-500, 0)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i69hLiKGpWm"
      },
      "source": [
        "Let's now see what did the algorithms learn by visualizing their actions at every state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHwk6jFiGpWm"
      },
      "outputs": [],
      "source": [
        "def draw_policy(env, agent):\n",
        "    \"\"\" Prints CliffWalkingEnv policy with arrows. Hard-coded. \"\"\"\n",
        "    n_rows, n_cols = env._cliff.shape\n",
        "\n",
        "    actions = '^>v<'\n",
        "\n",
        "    for yi in range(n_rows):\n",
        "        for xi in range(n_cols):\n",
        "            if env._cliff[yi, xi]:\n",
        "                print(\" C \", end='')\n",
        "            elif (yi * n_cols + xi) == env.start_state_index:\n",
        "                print(\" X \", end='')\n",
        "            elif (yi * n_cols + xi) == n_rows * n_cols - 1:\n",
        "                print(\" T \", end='')\n",
        "            else:\n",
        "                print(\" %s \" %\n",
        "                      actions[agent.get_best_action(yi * n_cols + xi)], end='')\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yUk6I6JGpWn"
      },
      "outputs": [],
      "source": [
        "print(\"Q-Learning\")\n",
        "draw_policy(env, agent_ql)\n",
        "\n",
        "print(\"SARSA\")\n",
        "draw_policy(env, agent_sarsa)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmJZhCkgGpWn"
      },
      "source": [
        "### Submit to Coursera"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixKsngyEGpWn"
      },
      "outputs": [],
      "source": [
        "from submit import submit_sarsa\n",
        "submit_sarsa(rewards_ql, rewards_sarsa, 'corey.taylor.de@gmail.com', 'NWRKUuLsNnNLhXB0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y-eMzaQGpWo"
      },
      "source": [
        "### More\n",
        "\n",
        "Here are some of the things you can do if you feel like it:\n",
        "\n",
        "* Play with epsilon. See learned how policies change if you set epsilon to higher/lower values (e.g. 0.75).\n",
        "* Expected Value SASRSA for softmax policy:\n",
        "$$ \\pi(a_i|s) = softmax({Q(s,a_i) \\over \\tau}) = {e ^ {Q(s,a_i)/ \\tau}  \\over {\\sum_{a_j}  e ^{Q(s,a_j) / \\tau }}} $$\n",
        "* Implement N-step algorithms and TD($\\lambda$): see [Sutton's book](http://incompleteideas.net/book/bookdraft2018jan1.pdf) chapter 7 and chapter 12.\n",
        "* Use those algorithms to train on CartPole in previous / next assignment for this week."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "sarsa.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}